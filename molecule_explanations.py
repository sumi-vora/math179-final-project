# -*- coding: utf-8 -*-
"""molecule_explanations.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NwtU-6t-9fPZqCqMHpcT4DKuvAVDGE-F
"""

!pip install rdkit

!pip install torch_geometric

!pip install cairosvg

from rdkit import Chem
from rdkit.Chem import rdmolops
import pandas as pd
from sklearn.preprocessing import OneHotEncoder
from scipy.sparse import coo_matrix
import numpy as np
from torch_geometric.data import Data
import torch
from tqdm import tqdm
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, ChebConv, global_mean_pool
from torch.autograd import Variable
from torch_geometric.data import DataLoader
import random
import matplotlib.pyplot as plt
from torch.optim.lr_scheduler import ReduceLROnPlateau
from rdkit.Chem import rdDepictor
from rdkit.Chem.Draw import rdMolDraw2D
import matplotlib
import matplotlib.cm as cm
from skimage.io import imread
import os
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from cairosvg import svg2png, svg2ps
from rdkit.Chem import rdDepictor
import warnings
warnings.filterwarnings("ignore")

# function for loading the data 
def load_bbbp(N=40):
    '''loads bbbp dataset'''
    print('Loading data...')
    df = pd.read_csv('BBBP.csv')
    feature_matrices = []  
    adj_matrices = []  
    labels = []  
    smiles_list = []
    nums = []
    for i in tqdm(range(len(df))): # use tqdm to create a progress bar 
        row = df.iloc[i] 
        nums.append(row.num) 
        smiles_list.append(row.smiles) 
        mol = Chem.MolFromSmiles(row.smiles) 
        if mol is None:
            continue

        # creating the adjacency matrix for each molecule 
        adj = rdmolops.GetAdjacencyMatrix(mol, useBO=True) 
        # this gets the adjacency matrix for the molecule 
        adj_matrix = np.zeros((N, N)) 
        s0, s1 = adj.shape 
        if s0 > N: # if it is bigger than 40 continue 
            continue
        adj_matrix[:s0, :s1] = adj # otherwise add the adjacency matrix to the list
        adj_matrices.append(adj_matrix)


        # creaating feature vectors for each molecule based on atomic number 
        atomic_nums = [atom.GetAtomicNum() for atom in mol.GetAtoms()]
        padded_atomic_nums = [0] * N # make sure all feature vectors are the same size
        padded_atomic_nums[:len(atomic_nums)] = atomic_nums # making feature matrix
        feature_matrices.append(padded_atomic_nums)

        # appending the labels for plotting 
        labels.append(row.p_np)

    # using one hot encoder to transform the matrices into numerical matrices 
    enc = OneHotEncoder(handle_unknown='ignore', sparse=False)
    one_hot_feature_matrices = enc.fit_transform(feature_matrices)
    one_hot_feature_matrices = np.reshape(one_hot_feature_matrices, (-1, N, 8))

    # making the dataset 
    dataset = []
    for i in range(len(labels)): 
        # turning everything into tensors 
        X = torch.from_numpy(one_hot_feature_matrices[i]).float()
        A = torch.from_numpy(adj_matrices[i]).float()
        y = torch.Tensor([[labels[i]]]).float()
        mol_num = torch.Tensor([nums[i]])
        # turning adjacency matrices into scipy format because they are sparse 
        A_coo = coo_matrix(A)
        # getting edge index and edge weight 
        edge_index = torch.from_numpy(np.vstack([A_coo.row, A_coo.col])).long()
        edge_weight = torch.from_numpy(A_coo.data).float()
        # putting everything into the dataset 
        dataset.append(Data(x=X,
                            edge_index=edge_index,
                            edge_attr=edge_weight,
                            y=y,
                            A=A,
                            mol_num=mol_num
                            ))

    return dataset

# creating a standard gcn 
class GCN(torch.nn.Module):
    def __init__(self, H_0, H_1, H_2, H_3):
        '''standard GCN for classification'''
        super(GCN, self).__init__()

        # variables for forward propagation 
        self.input = None 
        self.final_conv_acts = None
        self.final_conv_grads = None

        # graph convolutional layers
        self.conv1 = GCNConv(H_0, H_1)
        self.conv2 = GCNConv(H_1, H_2)
        self.conv3 = GCNConv(H_2, H_3)

        # last fully connected layer used for classification 
        self.fc1 = torch.nn.Linear(H_3, 1)

    def activations_hook(self, grad):
        ''' gets the gradients of the last convolutional layer'''
        self.final_conv_grads = grad

    def forward(self, data):
        '''forward pass of the GCN'''
        # getting features, edges, and edge weights from the data 
        h0, edge_index, edge_weight = data.x, data.edge_index, data.edge_attr
        h0.requires_grad = True
        self.input = h0
        # apply the first and second graph convolutional layer with relu activation
        h1 = F.relu(self.conv1(h0, edge_index, edge_weight))
        h2 = F.relu(self.conv2(h1, edge_index, edge_weight))

        # compute the activations of the final convolutional layer
        with torch.enable_grad():
            self.final_conv_acts = self.conv3(h2, edge_index, edge_weight)
        self.final_conv_acts.register_hook(self.activations_hook)
        h3 = F.relu(self.final_conv_acts)
        h4 = global_mean_pool(h3, data.batch) # global mean pooling
        out = torch.nn.Sigmoid()(self.fc1(h4)) # sigmoid function 
        return out

# defining hyperparameters 
epochs = 50
N = 40
H_0 = 8
H_1 = 128
H_2 = 256
H_3 = 512
train_frac = 0.8
batch_size = 32
shuffle_seed = 0
learning_rate = 0.02

# load and shuffle the dataset 
dataset = load_bbbp(N)
random.Random(shuffle_seed).shuffle(dataset)

# splitting into training and testing datasets 
split_idx = int(np.floor(len(dataset)*train_frac))
train_dataset = dataset[:split_idx]
test_dataset = dataset[split_idx:]

# mini-batch sampling 
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# loading the model 
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = GCN(H_0, H_1, H_2, H_3).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# training function
def train(loader):
    model.train()
    total_loss = 0
    for data in loader:
        data = data.to(device)
        optimizer.zero_grad() # reset gradients to zero 
        out = model(data) 
        loss = F.binary_cross_entropy(out, data.y) # binary cross-entropy loss 
        loss.backward() 
        total_loss += loss.item() * data.num_graphs
        optimizer.step() # update parameters 
    return total_loss / len(loader.dataset) # return average loss 


# testing function: returns test_loss and test_acc 
def test(loader):
    model.eval()
    correct = 0
    total_loss = 0
    for data in loader:
        data = data.to(device)
        with torch.no_grad():
            output = model(data)
            loss = F.binary_cross_entropy(output, data.y)
            total_loss += loss.item() * data.num_graphs
            pred_y = (output >= 0.5).float() # converting into binary values 
        correct += torch.sum(data.y == pred_y).item() # count number of correct predictions
    test_acc = correct / len(loader.dataset) # compute accuracy 
    test_loss = total_loss / len(loader.dataset) # compute loss 
    return test_loss, test_acc

# initializing train losses and test losses 
train_losses = []
test_losses = []
test_accs = []

# iteratively training the model 
for epoch in range(1, epochs + 1):
    loss = train(train_loader)
    train_losses.append(loss)
    test_loss, test_acc = test(test_loader)
    test_losses.append(test_loss)
    test_accs.append(test_acc)
    print(f'Epoch {epoch}, Loss: {loss:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}')

    # plotting the loss and accuracy 
    if epoch % 10 == 0:
        plt.plot(train_losses)
        plt.title('Loss vs Epoch')
        plt.ylabel('Loss')
        plt.xlabel('Epoch')
        plt.savefig('loss_vs_epoch')
        plt.close()

        plt.plot(train_losses, label='Train Loss')
        plt.plot(test_losses, label='Test Loss')
        plt.title('Loss vs Epoch')
        plt.ylabel('Loss')
        plt.xlabel('Epoch')
        plt.legend()
        plt.savefig('train_test_loss_vs_epoch')
        plt.close()

        plt.plot(test_accs)
        plt.title('Test Accuracy vs Epoch')
        plt.ylabel('Test Accuracy')
        plt.xlabel('Epoch')
        plt.savefig('test_acc_vs_epoch')
        plt.close()

torch.save(model.state_dict(), 'gcn_state_dict.pt') # saving the model to a dictionary

df = pd.read_csv('BBBP.csv')

def img_for_mol(mol, atom_weights=[]):
    ''' draws an image from a molecule''' 
    # highlighting parameters based on weights 
    highlight_kwargs = {}
    if len(atom_weights) > 0:
        norm = matplotlib.colors.Normalize(vmin=-1, vmax=1)
        cmap = cm.get_cmap('bwr')
        plt_colors = cm.ScalarMappable(norm=norm, cmap=cmap)
        atom_colors = {
            i: plt_colors.to_rgba(atom_weights[i]) for i in range(len(atom_weights))
        }
        highlight_kwargs = {
            'highlightAtoms': list(range(len(atom_weights))),
            'highlightBonds': [],
            'highlightAtomColors': atom_colors
        }

    # draw the coordinates for the molecule 
    rdDepictor.Compute2DCoords(mol)
    drawer = rdMolDraw2D.MolDraw2DSVG(280, 280)
    drawer.SetFontSize(6)

    mol = rdMolDraw2D.PrepareMolForDrawing(mol)
    drawer.DrawMolecule(mol, **highlight_kwargs)
    drawer.FinishDrawing()
    svg = drawer.GetDrawingText()
    svg = svg.replace('svg:', '') # convert svg into png so we can save it 
    svg2png(bytestring=svg, write_to='tmp.png', dpi=100)
    img = imread('tmp.png')
    os.remove('tmp.png')
    return img

def plot_explanations(model, data):
    ''' plots the explanations for molecules'''
    mol_num = int(data.mol_num.item()) # gets the index 
    row = df.iloc[mol_num] # gets the row 
    smiles = row.smiles # gets molecule and turns it into a Mol object 
    mol = Chem.MolFromSmiles(smiles)

    # plot formatting 
    fig, axes = plt.subplots(2, 3, figsize=(12, 8))
    axes[0][0].imshow(img_for_mol(mol)) # plotting the original molecule 
    axes[0][0].set_title(row['name']) # title is the molecule name 

    # plotting adjacency matrix 
    axes[0][1].set_title('Adjacency Matrix')
    axes[0][1].imshow(data.A.cpu().numpy())

    # plotting feature matrix 
    axes[0][2].set_title('Feature Matrix')
    axes[0][2].imshow(data.x.cpu().detach().numpy())

    # plotting saliency map 
    axes[1][0].set_title('Saliency Map')
    input_grads = model.input.grad.view(40, 8) # takes the gradient of output w respect to input
    saliency_map_weights = saliency_map(input_grads)[:mol.GetNumAtoms()] # sets the weights  
    scaled_saliency_map_weights = MinMaxScaler(feature_range=(0,1)).fit_transform(np.array(saliency_map_weights).reshape(-1, 1)).reshape(-1, )
    axes[1][0].imshow(img_for_mol(mol, atom_weights=scaled_saliency_map_weights))

    # grad-cam 
    axes[1][1].set_title('Grad-CAM')
    final_conv_acts = model.final_conv_acts.view(40, 512) # final layer 
    final_conv_grads = model.final_conv_grads.view(40, 512) # gradient-based weights 
    grad_cam_weights = grad_cam(final_conv_acts, final_conv_grads)[:mol.GetNumAtoms()] # use grad-cam function
    scaled_grad_cam_weights = MinMaxScaler(feature_range=(0,1)).fit_transform(np.array(grad_cam_weights).reshape(-1, 1)).reshape(-1, )
    axes[1][1].imshow(img_for_mol(mol, atom_weights=scaled_grad_cam_weights))

    # expectation backpropagation (EB)
    axes[1][2].set_title('EB')
    eb_weights = eb(mol, final_conv_acts, final_conv_grads)
    axes[1][2].imshow(img_for_mol(mol, atom_weights=eb_weights))


def saliency_map(input_grads):
    node_saliency_map = []
    for n in range(input_grads.shape[0]): # nth node
        node_grads = input_grads[n,:]  # extract the gradients for the current node
        node_saliency = torch.norm(F.relu(node_grads)).item() # apply relu activation and calculate l2 norm
        node_saliency_map.append(node_saliency)
    return node_saliency_map

def grad_cam(final_conv_acts, final_conv_grads):
    node_heat_map = []
    alphas = torch.mean(final_conv_grads, axis=0) # mean gradient for each feature (512x1)
    for n in range(final_conv_acts.shape[0]): # nth node
        node_heat = F.relu(alphas @ final_conv_acts[n]).item() # calculate the heat map value by taking the dot product between the ReLU-activated mean gradients and the activations for the current node
        node_heat_map.append(node_heat)
    return node_heat_map

def eb(mol, final_conv_acts, final_conv_grads):
    node_heat_map = []
    alphas = torch.mean(final_conv_grads, axis=0) # mean gradient for each feature (512x1)
    for n in range(final_conv_acts.shape[0]): # nth node
        node_heat = (alphas @ final_conv_acts[n]).item() # compute the importance scores by taking the dot product of activations and normalized gradients
        node_heat_map.append(node_heat)
    # reshape the importance scores to match the molecule shape
    node_heat_map = np.array(node_heat_map[:mol.GetNumAtoms()]).reshape(-1, 1)
    pos_node_heat_map = MinMaxScaler(feature_range=(0,1)).fit_transform(node_heat_map*(node_heat_map >= 0)).reshape(-1,)
    neg_node_heat_map = MinMaxScaler(feature_range=(-1,0)).fit_transform(node_heat_map*(node_heat_map < 0)).reshape(-1,)
    return pos_node_heat_map + neg_node_heat_map

# loading datasets and training model 
dataset = load_bbbp(N)
random.Random(shuffle_seed).shuffle(dataset)
split_idx = int(np.floor(len(dataset)*train_frac))
test_dataset = dataset[split_idx:]
loader = DataLoader(test_dataset, batch_size=1, shuffle=False)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = GCN(H_0, H_1, H_2, H_3).to(device)
model.load_state_dict(torch.load('gcn_state_dict.pt'))
model.eval()

optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

model.train()
total_loss = 0

# for each molecule, show the explainability graphs 
for data in tqdm(loader):
    data = data.to(device)
    optimizer.zero_grad()
    out = model(data)
    loss = F.binary_cross_entropy(out, data.y)
    loss.backward()
    try:
        plot_explanations(model, data)
    except Exception as e:
        print(e)
        continue
    total_loss += loss.item() * data.num_graphs